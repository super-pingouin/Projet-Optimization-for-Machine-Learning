{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26eb4cb",
   "metadata": {},
   "source": [
    "# Project : Optimization for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e854cb1-40b0-4b85-bcb8-46a133038957",
   "metadata": {},
   "source": [
    "Comparison script for the impact of learning rate on the convergence of different optimizers.\n",
    "\n",
    "Tested optimizers: Adam, RMSprop, AdaGrad, AdamW, AmsGrad, SGD, NAdam, RAdam\n",
    "Objective: Study the robustness of models to different learning rates\n",
    "Analyzed metrics: loss, accuracy, convergence speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff73a4c",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e80dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9142260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility across torch, numpy, and random.\n",
    "    Args: seed (int): The seed value to use.\n",
    "    This function ensures deterministic behavior across different runs by:\n",
    "    - Setting seeds for torch, numpy, and Python's random module\n",
    "    - Configuring PyTorch's cuDNN backend for reproducibility\n",
    "    - Setting the seed for all CUDA devices if available\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    Set the seed for data loader worker processes to ensure deterministic behavior.\n",
    "    Args: worker_id (int): Unique ID of the worker process.\n",
    "    This function is used when initializing PyTorch DataLoader workers with `worker_init_fn`.\n",
    "    \"\"\"\n",
    "    worker_seed = worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73252eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples processed in each training batch\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Total number of training epochs\n",
    "EPOCHS = 20\n",
    "\n",
    "# Automatically select GPU if available, otherwise fallback to CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2df782",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing and CNN benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db630709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define a transformation pipeline:\n",
    "    # - Convert PIL images to tensors\n",
    "    # - Normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# Load the CIFAR-10 training dataset with transformations applied\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Load the CIFAR-10 test dataset with the same transformations\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Set manual seed for the data loader generator to ensure reproducibility of shuffling\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "# Create the data loader for the training dataset \n",
    "# - Use seeded generator for reproducible shuffling\n",
    "# - Initialize each worker deterministically\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g, worker_init_fn=seed_worker)\n",
    "# Create the data loader for the test dataset (no shuffling needed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec5afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_model, self).__init__()\n",
    "        \n",
    "        # Feature extractor: two convolutional blocks with ReLU and max-pooling\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # First conv layer: input channels = 3 (RGB), output = 64\n",
    "            nn.ReLU(),                                  # Non-linearity\n",
    "            nn.MaxPool2d(2),                            # Downsample by factor of 2\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # Second conv layer: output = 128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                             # Downsample again\n",
    "        )\n",
    "        \n",
    "        # Classifier: flatten the features and pass through two fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                               # Flatten the 128×8×8 feature maps\n",
    "            nn.Linear(128 * 8 * 8, 256),                # Fully connected layer with 256 hidden units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)                          # Output layer for 10 CIFAR-10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8580bb",
   "metadata": {},
   "source": [
    "### Training and testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720db841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, batchnorm, etc.)\n",
    "    \n",
    "    correct = 0   # Counter for correct predictions\n",
    "    total = 0     # Counter for total samples\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Move data to the correct device (CPU or GPU)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability for each sample\n",
    "            total += targets.size(0)  # Update total number of samples\n",
    "            correct += (predicted == targets).sum().item()  # Count how many predictions were correct\n",
    "\n",
    "    accuracy = correct / total  # Compute final accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8c9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs, train_loader):\n",
    "    \"\"\"\n",
    "    Train the model over a number of epochs while tracking training loss \n",
    "    and test accuracy at each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer to update model parameters.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        epochs (int): Number of training epochs.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "\n",
    "    Returns:\n",
    "        losses (list of float): Average training loss per epoch (including initial).\n",
    "        acc_history (list of float): Test set accuracy after each epoch (including initial).\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()  # Set model to training mode (enables dropout, batchnorm, etc.)\n",
    "\n",
    "    losses = []       # To record average loss per epoch\n",
    "    acc_history = []  # To record accuracy on test set after each epoch\n",
    "\n",
    "    # Compute and store the initial loss before training starts\n",
    "    initial_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Move data to device\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Calculate loss\n",
    "        initial_loss += loss.item()  # Accumulate loss over batches\n",
    "    avg_initial_loss = initial_loss / len(train_loader)  # Average loss over all batches\n",
    "    losses.append(avg_initial_loss)  # Append initial loss to losses list\n",
    "\n",
    "    # Evaluate and store the initial accuracy on the test set before training\n",
    "    initial_accuracy = test(model, test_loader)\n",
    "    acc_history.append(initial_accuracy)\n",
    "\n",
    "    # Training loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Accumulate total loss for this epoch\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)  # Move data to device\n",
    "            optimizer.zero_grad()  # Clear gradients before backward pass\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "            total_loss += loss.item()  # Accumulate loss for this batch\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)  # Compute average loss for the epoch\n",
    "        losses.append(avg_loss)  # Store average loss\n",
    "\n",
    "        # Evaluate and record accuracy on the test set after this epoch\n",
    "        acc_history.append(test(model, test_loader))\n",
    "\n",
    "        # Uncomment to print loss progress per epoch\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses, acc_history  # Return lists of losses and accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897151c",
   "metadata": {},
   "source": [
    "### Comparison of optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cca2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of learning rates to explore during experiments\n",
    "learning_rates = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "# List of random seeds to ensure reproducibility and enable averaging over runs\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "# Dictionary mapping optimizer names to their initialization functions\n",
    "optimizers_dict = {\n",
    "    'Adam': lambda model, lr: optim.Adam(model.parameters(), lr=lr),\n",
    "    'RMSprop': lambda model, lr: optim.RMSprop(model.parameters(), lr=lr), #0.01\n",
    "    'AdaGrad': lambda model, lr: optim.Adagrad(model.parameters(), lr=lr), #0.01\n",
    "    'AdamW': lambda model, lr: optim.AdamW(model.parameters(), lr=lr), #0.001\n",
    "    'AmsGrad': lambda model, lr: optim.Adam(model.parameters(), lr=lr, amsgrad=True), #0.001\n",
    "    'NAdam' : lambda model, lr : optim.NAdam(model.parameters(), lr=lr), #0.002\n",
    "    'RAdam' : lambda model, lr : optim.RAdam(model.parameters(), lr=lr), #0.001\n",
    "    'SGD' : lambda model, lr : optim.SGD(model.parameters(), lr=lr), #0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b29277-5228-4849-8fb4-aa147b5c075b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd89cad4415a4ea7bdbbae17dfeba633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Testing optimizer: Adam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3e456692014f41a6a9cb41bc217902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: RMSprop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a8e586153f46bda01cff42f8d1b9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: AdaGrad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df86da82be747818ae17a1590810c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: AdamW\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e86b5f2b04b0ca0b3d21f62b11e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: AmsGrad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655932e0ba6a4eeb9e43606abc066a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: NAdam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bfb3d41a6742548dd12e17145b9685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: RAdam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643fd837a5864464a50ae5b7393490b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n",
      "\n",
      ">>> Testing optimizer: SGD\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f637f57d773451b94e58516db1fdd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LR = 1e-05\n",
      "  - LR = 0.0001\n",
      "  - LR = 0.001\n",
      "  - LR = 0.01\n",
      "  - LR = 0.1\n"
     ]
    }
   ],
   "source": [
    "# Initialize a nested dictionary to store results for each optimizer and learning rate\n",
    "results = defaultdict(dict)\n",
    "\n",
    "# Loop over each optimizer and its corresponding constructor function\n",
    "for opt_name, opt_fn in tqdm(optimizers_dict.items()):\n",
    "    print(f\"\\n>>> Testing optimizer: {opt_name}\")\n",
    "    \n",
    "    # Loop over the defined learning rates\n",
    "    for lr in tqdm(learning_rates):\n",
    "        print(f\"  - LR = {lr}\")\n",
    "\n",
    "        # Initialize the storage for accuracies and losses for all seeds at this optimizer/lr combo\n",
    "        results[opt_name][lr] = {\n",
    "            'accuracies': [],\n",
    "            'losses': []\n",
    "        }\n",
    "\n",
    "        # Run training for each seed to average out randomness\n",
    "        for seed in seeds:\n",
    "            set_seed(seed)  # Set random seeds for reproducibility\n",
    "            model = CNN_model().to(DEVICE)  # Instantiate and move model to device\n",
    "            optimizer = opt_fn(model, lr)   # Create optimizer with given lr\n",
    "            criterion = nn.CrossEntropyLoss()  # Define loss function (cross-entropy for classification)\n",
    "\n",
    "            # Train the model and get loss and accuracy history\n",
    "            losses, acc_history = train(model, optimizer, criterion, EPOCHS, train_loader)\n",
    "    \n",
    "            # Append the accuracy and loss history for this seed\n",
    "            results[opt_name][lr]['accuracies'].append(acc_history)\n",
    "            results[opt_name][lr]['losses'].append(losses)\n",
    "\n",
    "    # Save intermediate results for each optimizer to a separate file to avoid data loss on long runs\n",
    "    with open(f'simple_studies/8model_20epoch_saved_loss/{opt_name}', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "\n",
    "# After all training runs, compute the mean and standard deviation across seeds for accuracy and loss\n",
    "for opt_name in results:\n",
    "    for lr in results[opt_name]:\n",
    "        accs_all_seeds = np.array(results[opt_name][lr]['accuracies'])\n",
    "        results[opt_name][lr]['mean_acc'] = accs_all_seeds.mean(axis=0)\n",
    "        results[opt_name][lr]['std_acc'] = accs_all_seeds.std(axis=0)\n",
    "\n",
    "        losses_all_seeds = np.array(results[opt_name][lr]['losses'])\n",
    "        mean_losses = losses_all_seeds.mean(axis=0)\n",
    "        std_losses = losses_all_seeds.std(axis=0)\n",
    "        results[opt_name][lr]['mean_losses'] = mean_losses\n",
    "        results[opt_name][lr]['std_losses'] = std_losses\n",
    "\n",
    "# Save the final aggregated results with means and stds for all optimizers and learning rates\n",
    "with open(f'simple_studies/8model_20epoch_saved_loss/final_losses', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
